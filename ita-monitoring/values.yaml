# Default values for ita-monitoring.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

nameOverride: ""

fullnameOverride: ""

commonLabels: {}
# scmhash: abc123
# myLabel: aakkmd

prometheus-operator:
  nameOverride: "ita-operator"

  ## Create default rules for monitoring the cluster
  ##
  defaultRules:
    create: true
    rules:
      alertmanager: true
      etcd: false
      general: true
      k8s: true # TODO: CHECK IT
      kubeApiserver: false
      kubePrometheusNodeAlerting: true
      kubePrometheusNodeRecording: true
      kubeScheduler: false
      kubernetesAbsent: true
      kubernetesApps: true
      kubernetesResources: true
      kubernetesStorage: true
      kubernetesSystem: true
      node: true
      prometheusOperator: true
      prometheus: true

  ## Provide custom recording or alerting rules to be deployed into the cluster.
  ##
  additionalPrometheusRules: []
  #  - name: my-rule-file
  #    groups:
  #      - name: my_group
  #        rules:
  #        - record: my_record
  #          expr: 100 * my_record

  ##
  global:
    rbac:
      create: true
      pspEnabled: true

  ## Configuration for alertmanager
  ## ref: https://prometheus.io/docs/alerting/alertmanager/
  ##
  alertmanager:

    ## Deploy alertmanager
    ##
    enabled: true

    ## Service account for Alertmanager to use.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
    ##
    serviceAccount:
      create: true
      name: ""

    ## Alertmanager configuration directives
    ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
    ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
    ##
    config:
      global:
        resolve_timeout: 5m
      route:
        group_by: ['job']
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 12h
        receiver: 'null'
        routes:
        - match:
            alertname: Watchdog
          receiver: 'null'
      receivers:
      - name: 'null'

    ## Alertmanager template files to format alerts
    ## ref: https://prometheus.io/docs/alerting/notifications/
    ##      https://prometheus.io/docs/alerting/notification_examples/
    ##
    templateFiles: {}
    #
    # An example template:
    #   template_1.tmpl: |-
    #       {{ define "cluster" }}{{ .ExternalURL | reReplaceAll ".*alertmanager\\.(.*)" "$1" }}{{ end }}
    #
    #       {{ define "slack.myorg.text" }}
    #       {{- $root := . -}}
    #       {{ range .Alerts }}
    #         *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`
    #         *Cluster:*  {{ template "cluster" $root }}
    #         *Description:* {{ .Annotations.description }}
    #         *Graph:* <{{ .GeneratorURL }}|:chart_with_upwards_trend:>
    #         *Runbook:* <{{ .Annotations.runbook }}|:spiral_note_pad:>
    #         *Details:*
    #           {{ range .Labels.SortedPairs }} • *{{ .Name }}:* `{{ .Value }}`
    #           {{ end }}

    ## Configuration for Alertmanager service
    ##
    service:
      type: ClusterIP

    ## If true, create a serviceMonitor for alertmanager
    ##
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""
      selfMonitor: true

    ## Settings affecting alertmanagerSpec
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#alertmanagerspec
    ##
    alertmanagerSpec:
      ## Standard object’s metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/api-conventions.md#metadata
      ## Metadata Labels and Annotations gets propagated to the Alertmanager pods.
      ##
      podMetadata: {}

      ## Image of Alertmanager
      ##
      image:
        repository: quay.io/prometheus/alertmanager
        tag: v0.16.2

      ## If true then the user will be responsible to provide a secret with alertmanager configuration
      ## So when true the config part will be ignored (including templateFiles) and the one in the secret will be used
      ##
      useExistingSecret: false

      ## Size is the expected size of the alertmanager cluster. The controller will eventually make the size of the
      ## running cluster equal to the expected size.
      replicas: 1

      ## Time duration Alertmanager shall retain data for. Default is '120h', and must match the regular expression
      ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).
      ##
      retention: 48h

      ## SecurityContext holds pod-level security attributes and common container settings.
      ## This defaults to non root user with uid 1000 and gid 2000.	*v1.PodSecurityContext	false
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
      ##
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000

  ## Using default values from https://github.com/helm/charts/blob/master/stable/grafana/values.yaml
  ##
  grafana:
    enabled: true

    ## Deploy default dashboards.
    ##
    defaultDashboardsEnabled: true

    adminPassword: tobe-ornot

    sidecar:
      dashboards:
        enabled: true
        label: grafana_dashboard
      datasources:
        enabled: true
        defaultDatasourceEnabled: true
        label: grafana_datasource

    ## If true, create a serviceMonitor for grafana
    ##
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""
      selfMonitor: true

  ## Component scraping the kube api server
  ##
  kubeApiServer:
    enabled: false

  ## Component scraping the kubelet and kubelet-hosted cAdvisor
  ##
  kubelet:
    enabled: true
    namespace: kube-system

  ## Component scraping the kube controller manager
  ##
  kubeControllerManager:
    enabled: false

  ## Component scraping coreDns. Use either this or kubeDns
  ##
  coreDns:
    enabled: false

  ## Component scraping kubeDns. Use either this or coreDns
  ##
  kubeDns:
    enabled: false

  ## Component scraping etcd
  ##
  kubeEtcd:
    enabled: false

  ## Component scraping kube scheduler
  ##
  kubeScheduler:
    enabled: false

  ## Component scraping kube state metrics
  ##
  kubeStateMetrics:
    enabled: true

  ## Configuration for kube-state-metrics subchart
  ##
  kube-state-metrics:
    rbac:
      create: true
    podSecurityPolicy:
      enabled: true

  ## Deploy node exporter as a daemonset to all nodes
  ##
  nodeExporter:
    enabled: true

    ## Use the value configured in prometheus-node-exporter.podLabels
    ##
    jobLabel: jobLabel

  ## Configuration for prometheus-node-exporter subchart
  ##
  prometheus-node-exporter:
    podLabels:
      ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards
      ##
      jobLabel: node-exporter
    extraArgs:
      - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+)($|/)
      - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$

  ## Manages Prometheus and Alertmanager components
  ##
  prometheusOperator:
    enabled: true

    ## Service account for Alertmanager to use.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
    ##
    serviceAccount:
      create: true
      name: ""

    ## Deploy CRDs used by Prometheus Operator.
    ##
    createCustomResource: true

    ## Customize CRDs API Group
    crdApiGroup: monitoring.coreos.com

    ## Attempt to clean up CRDs created by Prometheus Operator.
    ##
    cleanupCustomResource: false

    ## If true, the operator will create and maintain a service for scraping kubelets
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/helm/prometheus-operator/README.md
    ##
    kubeletService:
      enabled: true
      namespace: kube-system

  ## Deploy a Prometheus instance
  ##
  prometheus:

    enabled: true

    ## Service account for Prometheuses to use.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
    ##
    serviceAccount:
      create: true
      name: ""

    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""
      selfMonitor: true

    ## Settings affecting prometheusSpec
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
    ##
    prometheusSpec:

      ## How long to retain metrics
      ##
      retention: 10d

      ## If true, the Operator won't process any Prometheus configuration changes
      ##
      paused: false

      ## Number of Prometheus replicas desired
      ##
      replicas: 1


      ## Prometheus StorageSpec for persistent data
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/storage.md
      ##
      storageSpec:
        volumeClaimTemplate:
          metadata:
            name: promdata
          spec:
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 10Gi
          selector: {}

      ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations
      ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form
      ## as specified in the official Prometheus documentation:
      ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#<scrape_config>. As scrape configs are
      ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility
      ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible
      ## scrape configs are going to break Prometheus after the upgrade.
      ##
      ## The scrape configuraiton example below will find master nodes, provided they have the name .*mst.*, relabel the
      ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes
      ##
      additionalScrapeConfigs: []
      # - job_name: kube-etcd
      #   kubernetes_sd_configs:
      #     - role: node
      #   scheme: https
      #   tls_config:
      #     ca_file:   /etc/prometheus/secrets/etcd-client-cert/etcd-ca
      #     cert_file: /etc/prometheus/secrets/etcd-client-cert/etcd-client
      #     key_file:  /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
      #   relabel_configs:
      #   - action: labelmap
      #     regex: __meta_kubernetes_node_label_(.+)
      #   - source_labels: [__address__]
      #     action: replace
      #     target_label: __address__
      #     regex: ([^:;]+):(\d+)
      #     replacement: ${1}:2379
      #   - source_labels: [__meta_kubernetes_node_name]
      #     action: keep
      #     regex: .*mst.*
      #   - source_labels: [__meta_kubernetes_node_name]
      #     action: replace
      #     target_label: node
      #     regex: (.*)
      #     replacement: ${1}
      #   metric_relabel_configs:
      #   - regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)
      #     action: labeldrop


      ## AdditionalAlertManagerConfigs allows for manual configuration of alertmanager jobs in the form as specified
      ## in the official Prometheus documentation https://prometheus.io/docs/prometheus/latest/configuration/configuration/#<alertmanager_config>.
      ## AlertManager configurations specified are appended to the configurations generated by the Prometheus Operator.
      ## As AlertManager configs are appended, the user is responsible to make sure it is valid. Note that using this
      ## feature may expose the possibility to break upgrades of Prometheus. It is advised to review Prometheus release
      ## notes to ensure that no incompatible AlertManager configs are going to break Prometheus after the upgrade.
      ##
      additionalAlertManagerConfigs: []
      # - consul_sd_configs:
      #   - server: consul.dev.test:8500
      #     scheme: http
      #     datacenter: dev
      #     tag_separator: ','
      #     services:
      #       - metrics-prometheus-alertmanager

      ## AdditionalAlertRelabelConfigs allows specifying Prometheus alert relabel configurations. Alert relabel configurations specified are appended
      ## to the configurations generated by the Prometheus Operator. Alert relabel configurations specified must have the form as specified in the
      ## official Prometheus documentation: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#alert_relabel_configs.
      ## As alert relabel configs are appended, the user is responsible to make sure it is valid. Note that using this feature may expose the
      ## possibility to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible alert relabel
      ## configs are going to break Prometheus after the upgrade.
      ##
      additionalAlertRelabelConfigs: []
      # - separator: ;
      #   regex: prometheus_replica
      #   replacement: $1
      #   action: labeldrop